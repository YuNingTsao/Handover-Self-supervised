{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import copy\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "from math import ceil\n",
    "from itertools import cycle\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training start, 总共 80 epochs\n",
      "GPUs: cuda\n",
      "DeeplabV3+ with ResNet 101 backbone\n",
      "Current Labeled Example: 515\n",
      "Learning rate: other 0.01, and head is the SAME [world]\n",
      "Current batch: 14 [world]\n",
      "Current unsupervised loss function: semi_ce, with weight 1.5 and length 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "config json :\n",
      "name PS-MT(DeeplabV3+)\n",
      "experim_name TEST_warm\n",
      "n_labeled_examples 515\n",
      "ramp_up 12\n",
      "unsupervised_w 1.5\n",
      "lr_scheduler Poly\n",
      "gamma 0.5\n",
      "model {'supervised': False, 'semi': True, 'resnet': 101, 'sup_loss': 'DE', 'un_loss': 'semi_ce', 'epochs': 80, 'warm_up_epoch': 100, 'data_h_w': [224, 224]}\n",
      "optimizer {'type': 'SGD', 'args': {'lr': 0.01, 'weight_decay': 0.0001, 'momentum': 0.9}}\n",
      "train_supervised {'data_dir': 'FA', 'batch_size': 7, 'shuffle': True, 'crop_size': 224, 'split': 'train_supervised', 'num_workers': 8}\n",
      "train_unsupervised {'data_dir': 'range_unFA', 'batch_size': 7, 'shuffle': True, 'crop_size': 224, 'split': 'train_unsupervised', 'num_workers': 8}\n",
      "warm_selfsupervised {'data_dir': 'range_unFA', 'batch_size': 7, 'shuffle': True, 'crop_size': 224, 'split': 'train_unsupervised', 'num_workers': 8}\n",
      "val_loader {'data_dir': 'FA', 'batch_size': 1, 'split': 'val', 'shuffle': False, 'num_workers': 4}\n",
      "test_loader {'data_dir': 'FA', 'batch_size': 1, 'split': 'test', 'shuffle': False, 'num_workers': 4}\n"
     ]
    }
   ],
   "source": [
    "#config\n",
    "batch_size = 7\n",
    "epochs = 80\n",
    "warm_up = 100\n",
    "labeled_examples = 515\n",
    "lr = 1e-2\n",
    "backbone = 101 #the resnet x {50, 101} layers\n",
    "semi_p_th = 0.6 # positive_threshold for semi-supervised loss\n",
    "semi_n_th = 0.0 # negative_threshold for semi-supervised loss\n",
    "unsup_weight = 1.5 # unsupervised weight for semi-supervised loss\n",
    "\n",
    "config = json.load(open(\"configs/config_deeplab_v3+_onlyFA_range_selfsupervised.json\"))\n",
    "\n",
    "config['train_supervised']['batch_size'] = batch_size\n",
    "config['train_unsupervised']['batch_size'] = batch_size\n",
    "config['warm_selfsupervised']['batch_size'] = batch_size\n",
    "config['model']['epochs'] = epochs\n",
    "config['model']['warm_up_epoch'] = warm_up\n",
    "config['n_labeled_examples'] = labeled_examples\n",
    "config['model']['resnet'] = backbone\n",
    "config['optimizer']['args']['lr'] = lr\n",
    "config['unsupervised_w'] = unsup_weight\n",
    "config['model']['data_h_w'] = [config['train_supervised']['crop_size'], config['train_supervised']['crop_size']]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logger = logging.getLogger(\"PS-MT\")\n",
    "logger.propagate = False\n",
    "logger.warning(\"Training start, 总共 {} epochs\".format(str(config['model']['epochs'])))\n",
    "logger.critical(\"GPUs: {}\".format(device))\n",
    "logger.critical(\"DeeplabV3+ with ResNet {} backbone\".format(str(config['model']['resnet'])))\n",
    "logger.critical(\"Current Labeled Example: {}\".format(config['n_labeled_examples']))\n",
    "logger.critical(\"Learning rate: other {}, and head is the SAME [world]\".format(config['optimizer']['args']['lr']))\n",
    "\n",
    "logger.critical(\"Current batch: {} [world]\".format(int(config['train_unsupervised']['batch_size']) +\n",
    "                                                int(config['train_supervised']['batch_size'])) )\n",
    "\n",
    "logger.critical(\"Current unsupervised loss function: {}, with weight {} and length {}\".format(config['model']['un_loss'],\n",
    "                                                                                            config['unsupervised_w'],\n",
    "                                                                                            config['ramp_up']))\n",
    "\n",
    "\n",
    "#Need to add self-supervised loss function info\n",
    "print(\"\\nconfig json :\")\n",
    "for i in config:\n",
    "    print(i, config[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_supervised\n",
      "     data_dir : FA\n",
      "     batch_size : 7\n",
      "     shuffle : True\n",
      "     crop_size : 224\n",
      "     split : train_supervised\n",
      "     num_workers : 8\n",
      "     choose : All\n",
      "train_unsupervised\n",
      "     data_dir : range_unFA\n",
      "     batch_size : 7\n",
      "     shuffle : True\n",
      "     crop_size : 224\n",
      "     split : train_unsupervised\n",
      "     num_workers : 8\n",
      "     choose : All\n",
      "warm_selfsupervised\n",
      "     data_dir : range_unFA\n",
      "     batch_size : 7\n",
      "     shuffle : True\n",
      "     crop_size : 224\n",
      "     split : train_unsupervised\n",
      "     num_workers : 8\n",
      "     choose : All\n",
      "val_loader\n",
      "     data_dir : FA\n",
      "     batch_size : 1\n",
      "     split : val\n",
      "     shuffle : False\n",
      "     num_workers : 4\n",
      "     choose : All\n",
      "test_loader\n",
      "     data_dir : FA\n",
      "     batch_size : 1\n",
      "     split : test\n",
      "     shuffle : False\n",
      "     num_workers : 4\n",
      "     choose : All\n",
      "supervised_loader:  515\n",
      "unsupervised_loader:  5263\n",
      "warm_selfsupervised_loader:  5263\n",
      "val_loader:  162\n",
      "test_loader:  157\n"
     ]
    }
   ],
   "source": [
    "# DATA LOADERS\n",
    "from DataLoader.dataset_onlyFA import *\n",
    "choose_data = \"All\"\n",
    "\n",
    "config['train_supervised']['choose'] = choose_data\n",
    "config['train_unsupervised']['choose'] = choose_data\n",
    "config['warm_selfsupervised']['choose'] = choose_data\n",
    "config['val_loader']['choose'] = choose_data\n",
    "config['test_loader']['choose'] = choose_data\n",
    "\n",
    "print(\"train_supervised\")\n",
    "for i in config['train_supervised']:\n",
    "    print(\"    \",i, \":\", config['train_supervised'][i])\n",
    "print(\"train_unsupervised\")\n",
    "for i in config['train_unsupervised']:\n",
    "    print(\"    \", i, \":\", config['train_unsupervised'][i])\n",
    "print(\"warm_selfsupervised\")\n",
    "for i in config['warm_selfsupervised']:\n",
    "    print(\"    \", i, \":\", config['warm_selfsupervised'][i])\n",
    "print(\"val_loader\")\n",
    "for i in config['val_loader']:\n",
    "    print(\"    \", i, \":\", config['val_loader'][i])\n",
    "print(\"test_loader\")\n",
    "for i in config['test_loader']:\n",
    "    print(\"    \", i, \":\", config['test_loader'][i])\n",
    "\n",
    "supervised_set = BasicDataset(data_dir=config['train_supervised']['data_dir'], \n",
    "                                 choose=config['train_supervised']['choose'],\n",
    "                                 split=config['train_supervised']['split'])\n",
    "\n",
    "unsupervised_set = BasicDataset(data_dir=config['train_unsupervised']['data_dir'],\n",
    "                                   choose=config['train_unsupervised']['choose'],\n",
    "                                   split=config['train_unsupervised']['split'])\n",
    "\n",
    "warm_selfsupervised_set = BasicDataset(data_dir=config['warm_selfsupervised']['data_dir'],\n",
    "                                      choose=config['warm_selfsupervised']['choose'],\n",
    "                                      split=config['warm_selfsupervised']['split'])\n",
    "\n",
    "val_set = BasicDataset(data_dir=config['val_loader']['data_dir'],\n",
    "                          choose=config['val_loader']['choose'],\n",
    "                          split=config['val_loader']['split'])\n",
    "\n",
    "test_set = BasicDataset(data_dir=config['test_loader']['data_dir'],\n",
    "                          choose=config['test_loader']['choose'],\n",
    "                          split=config['test_loader']['split'])\n",
    "\n",
    "print(\"supervised_loader: \",len(supervised_set))\n",
    "print(\"unsupervised_loader: \",len(unsupervised_set))\n",
    "print(\"warm_selfsupervised_loader: \",len(warm_selfsupervised_set))\n",
    "print(\"val_loader: \",len(val_set))\n",
    "print(\"test_loader: \",len(test_set))\n",
    "\n",
    "\n",
    "supervised_loader = DataLoader(dataset=supervised_set, batch_size=config['train_supervised']['batch_size'],\n",
    "                               shuffle=config['train_supervised']['shuffle'], \n",
    "                               num_workers=config['train_supervised']['num_workers'])\n",
    "\n",
    "unsupervised_loader = DataLoader(dataset=unsupervised_set, batch_size=config['train_unsupervised']['batch_size'],\n",
    "                               shuffle=config['train_unsupervised']['shuffle'], \n",
    "                               num_workers=config['train_unsupervised']['num_workers'])\n",
    "                               \n",
    "warm_selfsupervised_loader = DataLoader(dataset=unsupervised_set, batch_size=config['warm_selfsupervised']['batch_size'],\n",
    "                              shuffle=config['warm_selfsupervised']['shuffle'],\n",
    "                              num_workers=config['warm_selfsupervised']['num_workers'])\n",
    "\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=config['val_loader']['batch_size'],\n",
    "                               shuffle=config['val_loader']['shuffle'], \n",
    "                               num_workers=config['val_loader']['num_workers'])\n",
    "\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=config['test_loader']['batch_size'],\n",
    "                               shuffle=config['test_loader']['shuffle'], \n",
    "                               num_workers=config['test_loader']['num_workers'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model setting (1 teacher + 1 student)\n",
    "from torch import optim\n",
    "\n",
    "from Utils.losses import ConsistencyWeight\n",
    "from Model.selfsupervised.selfsupervised_model import *\n",
    "from Utils.ramps import *\n",
    "\n",
    "cons_w_unsup = ConsistencyWeight(final_w=config['unsupervised_w'], iters_per_epoch=len(unsupervised_loader),\n",
    "                                 rampup_starts=0, rampup_ends=config['ramp_up'],  ramp_type=\"cosine_rampup\")\n",
    "\n",
    "\n",
    "model_t1 = Teacher_Net(num_classes=2, config=config['model'])\n",
    "model_t1 = model_t1.to(device)\n",
    "\n",
    "model_s = Student_Net(num_classes=2, config=config['model'],  cons_w_unsup=cons_w_unsup)\n",
    "model_s = model_s.to(device)\n",
    "\n",
    "optimizer_t1 = optim.SGD(model_t1.parameters(), \n",
    "                      lr=config['optimizer']['args']['lr'],\n",
    "                      momentum=config['optimizer']['args']['momentum'],\n",
    "                      weight_decay=config['optimizer']['args']['weight_decay'])\n",
    "\n",
    "optimizer_s = optim.SGD(model_s.parameters(), \n",
    "                      lr=config['optimizer']['args']['lr'],\n",
    "                      momentum=config['optimizer']['args']['momentum'],\n",
    "                      weight_decay=config['optimizer']['args']['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate metrics\n",
    "from sklearn import metrics, neighbors\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#compute mean IoU & DSC  of given outputs & targets per patient (5 time points)\n",
    "def information_index(outputs, targets):\n",
    "    eps = np.finfo(np.float64).eps\n",
    "    output = outputs.flatten()\n",
    "    target = targets.flatten()\n",
    "    \n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(target, output).ravel()\n",
    "    \n",
    "    # Handle different shapes of confusion matrix\n",
    "    if len(cm) == 4:\n",
    "        TN, FP, FN, TP = cm\n",
    "    elif len(cm) == 1:\n",
    "        # Case where only one class is present in both target and output\n",
    "        if target[0] == 0:\n",
    "            TN, FP, FN, TP = cm[0], 0, 0, 0  # All True Negatives\n",
    "        else:\n",
    "            TN, FP, FN, TP = 0, 0, 0, cm[0]  # All True Positives\n",
    "    elif len(cm) == 2:\n",
    "        # Case where target and output contain only a single class\n",
    "        if target[0] == 0:\n",
    "            TN, FP, FN, TP = cm[0], cm[1], 0, 0  # No False Negatives or True Positives\n",
    "        else:\n",
    "            TN, FP, FN, TP = 0, 0, cm[0], cm[1]  # No True Negatives or False Positives\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected confusion matrix size.\")\n",
    "\n",
    "    # Compute IoU and Dice coefficients\n",
    "    index_MIou = (TP / (TP + FP + FN + eps) + TN / (TN + FN + FP + eps)) / 2\n",
    "    mean_iou = np.mean(index_MIou)\n",
    "    index_dice = 2 * TP / (2 * TP + FP + FN + eps)\n",
    "    mean_dice = np.mean(index_dice)\n",
    "\n",
    "    return mean_iou, mean_dice\n",
    "#compute mean IoU & DSC of validaton (test set)\n",
    "def count_index(pre, tar):\n",
    "        path_pre = pre\n",
    "        path_target = tar\n",
    "        dirs = os.listdir(path_pre)\n",
    "        # print(len(dirs))\n",
    "        con_mIOU = 0\n",
    "        con_mdice = 0\n",
    "        for imgs in dirs:\n",
    "            pre_path = path_pre + '/' + str(imgs)\n",
    "            target_path = path_target + '/' + str(imgs)\n",
    "\n",
    "            target = cv2.imread(target_path, cv2.IMREAD_GRAYSCALE)\n",
    "            _, tar = cv2.threshold(target, 128, 1, cv2.THRESH_BINARY)\n",
    "\n",
    "            predict = cv2.imread(pre_path, cv2.IMREAD_GRAYSCALE)\n",
    "            _, pre = cv2.threshold(predict, 128, 1, cv2.THRESH_BINARY)\n",
    "\n",
    "            tIOU, tdice = information_index(pre,tar)\n",
    "            con_mIOU += tIOU\n",
    "            con_mdice += tdice\n",
    "        val_mIoU = con_mIOU/len(dirs)\n",
    "        val_mDice = con_mdice/len(dirs)\n",
    "        \n",
    "        return val_mIoU, val_mDice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 --- < Starting Time : Mon Sep 23 22:57:32 2024 >\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:13<00:00,  5.63it/s]\n",
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1 loss : \n",
      "teacher 1 loss = 0.044037364423274994,student loss = 0.04407580941915512\n",
      "Epoch: 2/100 --- < Starting Time : Mon Sep 23 23:00:38 2024 >\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:36<00:00,  4.81it/s]\n",
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch2 loss : \n",
      "teacher 1 loss = 0.04334758594632149,student loss = 0.04333049803972244\n",
      "Epoch: 3/100 --- < Starting Time : Mon Sep 23 23:04:10 2024 >\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:41<00:00,  4.67it/s]\n",
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch3 loss : \n",
      "teacher 1 loss = 0.04336659237742424,student loss = 0.043339282274246216\n",
      "Epoch: 4/100 --- < Starting Time : Mon Sep 23 23:07:48 2024 >\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:43<00:00,  4.59it/s]\n",
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch4 loss : \n",
      "teacher 1 loss = 0.043422769755125046,student loss = 0.043397240340709686\n",
      "Epoch: 5/100 --- < Starting Time : Mon Sep 23 23:11:27 2024 >\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:44<00:00,  4.57it/s]\n",
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch5 loss : \n",
      "teacher 1 loss = 0.04341918230056763,student loss = 0.043406181037425995\n",
      "Epoch: 6/100 --- < Starting Time : Mon Sep 23 23:15:09 2024 >\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:44<00:00,  4.57it/s]\n",
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch6 loss : \n",
      "teacher 1 loss = 0.04336138814687729,student loss = 0.04329809918999672\n",
      "Epoch: 7/100 --- < Starting Time : Mon Sep 23 23:18:50 2024 >\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:44<00:00,  4.57it/s]\n",
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch7 loss : \n",
      "teacher 1 loss = 0.04340789467096329,student loss = 0.0433475598692894\n",
      "Epoch: 8/100 --- < Starting Time : Mon Sep 23 23:22:31 2024 >\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:46<00:00,  4.51it/s]\n",
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch8 loss : \n",
      "teacher 1 loss = 0.04333417862653732,student loss = 0.04327346384525299\n",
      "Epoch: 9/100 --- < Starting Time : Mon Sep 23 23:26:13 2024 >\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:46<00:00,  4.52it/s]\n",
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch9 loss : \n",
      "teacher 1 loss = 0.0433523990213871,student loss = 0.0432744026184082\n",
      "Epoch: 10/100 --- < Starting Time : Mon Sep 23 23:29:57 2024 >\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:45<00:00,  4.54it/s]\n",
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch10 loss : \n",
      "teacher 1 loss = 0.04330949857831001,student loss = 0.04316982626914978\n",
      "Epoch: 11/100 --- < Starting Time : Mon Sep 23 23:33:39 2024 >\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:45<00:00,  4.55it/s]\n",
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch11 loss : \n",
      "teacher 1 loss = 0.043358657509088516,student loss = 0.04261606186628342\n",
      "Epoch: 12/100 --- < Starting Time : Mon Sep 23 23:37:21 2024 >\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:44<00:00,  4.58it/s]\n",
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch12 loss : \n",
      "teacher 1 loss = 0.04324383661150932,student loss = 0.028575142845511436\n",
      "Epoch: 13/100 --- < Starting Time : Mon Sep 23 23:41:01 2024 >\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:51<00:00,  4.39it/s]\n",
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch13 loss : \n",
      "teacher 1 loss = 0.043027911335229874,student loss = 0.024160167202353477\n",
      "Epoch: 14/100 --- < Starting Time : Mon Sep 23 23:44:53 2024 >\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:45<00:00,  4.55it/s]\n",
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch14 loss : \n",
      "teacher 1 loss = 0.03464961796998978,student loss = 0.022775663062930107\n",
      "Epoch: 15/100 --- < Starting Time : Mon Sep 23 23:48:38 2024 >\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:51<00:00,  4.39it/s]\n",
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch15 loss : \n",
      "teacher 1 loss = 0.024566488340497017,student loss = 0.021190933883190155\n",
      "Epoch: 16/100 --- < Starting Time : Mon Sep 23 23:52:30 2024 >\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:53<00:00,  4.33it/s]\n",
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch16 loss : \n",
      "teacher 1 loss = 0.0231124609708786,student loss = 0.018612105399370193\n",
      "Epoch: 17/100 --- < Starting Time : Mon Sep 23 23:56:23 2024 >\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:50<00:00,  4.41it/s]\n",
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch17 loss : \n",
      "teacher 1 loss = 0.021575195714831352,student loss = 0.01592775247991085\n",
      "Epoch: 18/100 --- < Starting Time : Tue Sep 24 00:00:11 2024 >\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:48<00:00,  4.45it/s]\n",
      "  0%|          | 0/752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch18 loss : \n",
      "teacher 1 loss = 0.01938559114933014,student loss = 0.014094416983425617\n",
      "Epoch: 19/100 --- < Starting Time : Tue Sep 24 00:03:58 2024 >\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [02:47<00:00,  4.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# warm\n",
    "import torch.nn.functional\n",
    "from Utils.early_stop import EarlyStopping\n",
    "\n",
    "early_stopper = EarlyStopping(patience=5, delta=0.0001)\n",
    "\n",
    "for epoch in range(config['model']['warm_up_epoch']):\n",
    "\n",
    "    epoch_loss_t1 = 0\n",
    "    epoch_loss_s = 0\n",
    "    t1_mIoU, t1_mDice = 0, 0\n",
    "    s_mIoU, s_mDice = 0, 0\n",
    "    \n",
    "    localtime = time.asctime( time.localtime(time.time()) )\n",
    "    print('Epoch: {}/{} --- < Starting Time : {} >'.format(epoch + 1,config['model']['warm_up_epoch'],localtime))\n",
    "    print('-' * len('Epoch: {}/{} --- < Starting Time : {} >'.format(epoch + 1,config['model']['warm_up_epoch'],localtime)))\n",
    "\n",
    "    folder_name = None\n",
    "    folder_name = os.path.join(\"see_image\", \"self-warm\")\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    for batch in tqdm(warm_selfsupervised_loader):\n",
    "        input_ul, target_ul, id_ul = batch\n",
    "        input_ul, target_ul, id_ul = input_ul.to(device), target_ul.to(device), id_ul\n",
    "\n",
    "\n",
    "        for i in range(0, int(target_ul.size(0))):\n",
    "            folder_target = os.path.join(folder_name, \"target\")\n",
    "            os.makedirs(folder_target, exist_ok=True)\n",
    "            image = Image.fromarray(np.int8(target_ul[i].detach().cpu().numpy()))\n",
    "            image.save(os.path.join(folder_target, str(id_ul[i]) + \".png\"))\n",
    "\n",
    "        # warm teacher\n",
    "        model_t1.train()\n",
    "        model_s.eval()\n",
    "        optimizer_t1.zero_grad()\n",
    "        loss_t1, outputs = model_t1(input_ul=input_ul, target_ul=input_ul, warm_up=True, mix_up=False)\n",
    "        output_t1 = outputs[\"self_pred\"]\n",
    "        \n",
    "\n",
    "        for i in range(0, int(output_t1.size(0))):\n",
    "            folder_t1_prob = os.path.join(folder_name, \"t1_prob\")\n",
    "            os.makedirs(folder_t1_prob, exist_ok=True)\n",
    "            image_prob = output_t1[i].squeeze().detach()\n",
    "            image_prob = torch.argmax(image_prob, dim=0).cpu().numpy()\n",
    "            image_prob = Image.fromarray((image_prob * 255).astype(np.uint8))\n",
    "            image_prob.save(os.path.join(folder_t1_prob, str(id_ul[i]) + \".png\"))\n",
    "\n",
    "\n",
    "        epoch_loss_t1 += loss_t1\n",
    "        loss_t1.backward()\n",
    "        optimizer_t1.step()\n",
    "        \n",
    "        # warm student\n",
    "        model_t1.eval()\n",
    "        model_s.train()\n",
    "        optimizer_s.zero_grad()\n",
    "        loss_s, outputs = model_s(x_ul=input_ul, warm_up=True, mix_up=False)\n",
    "        output_s = outputs[\"self_pred\"]\n",
    "        \n",
    "        for i in range(0, int(output_s.size(0))):\n",
    "            folder_s_prob = os.path.join(folder_name, \"s_prob\")\n",
    "            os.makedirs(folder_s_prob, exist_ok=True)\n",
    "            image_prob = output_s[i].squeeze().detach()\n",
    "            image_prob = torch.argmax(image_prob, dim=0).cpu().numpy()\n",
    "            image_prob = Image.fromarray((image_prob * 255).astype(np.uint8))\n",
    "            image_prob.save(os.path.join(folder_s_prob, str(id_ul[i]) + \".png\"))\n",
    "\n",
    "        epoch_loss_s += loss_s\n",
    "        loss_s.backward()\n",
    "        optimizer_s.step()\n",
    "\n",
    "    t1_mIoU, t1_mDice = count_index(folder_t1_prob, folder_target)\n",
    "    s_mIoU, s_mDice = count_index(folder_s_prob, folder_target)\n",
    "\n",
    "    print(f'Epoch{epoch+1} loss : \\nteacher 1 loss = {epoch_loss_t1/len(supervised_set)},student loss = {epoch_loss_s/len(supervised_set)}') \n",
    "\n",
    "\n",
    "    if s_mDice > 0.95:\n",
    "        early_stopper(epoch_loss_s/len(supervised_set))\n",
    "        print(f'teacher 1 : mIoU = {t1_mIoU}, DCS = {t1_mDice}')\n",
    "        print(f'student : mIoU = {s_mIoU}, DCS = {s_mDice}')\n",
    "        if early_stopper.early_stop: \n",
    "            print(\"Early stopping\")   \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep the parameters of teacher unchanged\n",
    "def freeze_teachers_parameters(model):\n",
    "    for p in model.maskAutoEncoder.encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in model.maskAutoEncoder.decoder.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "freeze_teachers_parameters(model_t1)\n",
    "\n",
    "# update teacher's parameters according to student's parameters\n",
    "def update_teachers(teacher, student, keep_rate=0.996):\n",
    "    student_encoder_dict = student.encoder.state_dict()\n",
    "    student_decoder_dict = student.decoder.state_dict()\n",
    "    new_teacher_encoder_dict = OrderedDict()\n",
    "    new_teacher_decoder_dict = OrderedDict()\n",
    "\n",
    "    for key, value in teacher.encoder.state_dict().items():\n",
    "\n",
    "        if key in student_encoder_dict.keys():\n",
    "            new_teacher_encoder_dict[key] = (\n",
    "                    student_encoder_dict[key] * (1 - keep_rate) + value * keep_rate\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\"{} is not found in student encoder model\".format(key))\n",
    "\n",
    "    for key, value in teacher.decoder.state_dict().items():\n",
    "\n",
    "        if key in student_decoder_dict.keys():\n",
    "            new_teacher_decoder_dict[key] = (\n",
    "                    student_decoder_dict[key] * (1 - keep_rate) + value * keep_rate\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\"{} is not found in student decoder model\".format(key))\n",
    "    teacher.encoder.load_state_dict(new_teacher_encoder_dict, strict=True)\n",
    "    teacher.decoder.load_state_dict(new_teacher_decoder_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semi train\n",
    "from Utils.early_stop import EarlyStopper\n",
    "\n",
    "early_stopper = EarlyStopper(patience=5, delta=0.001)\n",
    "\n",
    "best_model_t1_params = copy.deepcopy(model_t1.state_dict())\n",
    "best_model_s_params = copy.deepcopy(model_s.state_dict())\n",
    "do_best_Dice = 0\n",
    "do_best_mIoU = 0\n",
    "# do_best_hd95 = 100000\n",
    "do_best_epoch = 0\n",
    "\n",
    "for epoch in range(config['model']['epochs']):\n",
    "\n",
    "    model_s.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_dsc = 0\n",
    "\n",
    "    dataloader = iter(zip(cycle(supervised_loader), unsupervised_loader))\n",
    "\n",
    "    localtime = time.asctime( time.localtime(time.time()) )\n",
    "    print('Epoch: {}/{} --- < Starting Time : {} >'.format(epoch + 1,config['model']['epochs'],localtime))\n",
    "    print('-' * len('Epoch: {}/{} --- < Starting Time : {} >'.format(epoch + 1,config['model']['epochs'],localtime)))\n",
    "\n",
    "    tbar = tqdm(range(len(unsupervised_loader)))\n",
    "    for batch_idx in tbar:\n",
    "        (image_FA, image_ICG, target_l, id_l), (input_ul, target_ul, id_ul) = next(dataloader)\n",
    "        image_FA, image_ICG, target_l, id_l = image_FA.to(device), image_ICG.to(device), target_l.to(device), id_l\n",
    "        input_ul, target_ul, id_ul = input_ul.to(device), target_ul.to(device), id_ul\n",
    "        input_ul = torch.nn.functional.interpolate(input_ul, size=(input_ul.shape[-2], input_ul.shape[-1]), mode='bilinear', align_corners=True)\n",
    "        \n",
    "        optimizer_s.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "           loss_t1, predict_target_ul1 = model_t1(input_ul=input_ul, target_ul=target_ul)\n",
    "           predict_target_ul1 = torch.nn.functional.interpolate(predict_target_ul1,\n",
    "                                                                    size=(input_ul.shape[-2], input_ul.shape[-1]),\n",
    "                                                                    mode='bilinear',\n",
    "                                                                    align_corners=True)\n",
    "           \n",
    "        total_loss, cur_losses, outputs = model_s(x_FA=image_FA, x_ICG=image_ICG, target_l=target_l,\n",
    "                                                  x_ul=input_ul, target_ul=predict_target_ul1,\n",
    "                                                  epoch=epoch, curr_iter=batch_idx, warm_up=False,\n",
    "                                                  mix_up=False, t1=model_t1, t2=model_t1)\n",
    "        \n",
    "        epoch_loss += total_loss\n",
    "        total_loss.backward()\n",
    "        optimizer_s.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            update_teachers(teacher=model_t1.segmentationMAE,\n",
    "                            student=model_s.segmentationMAE)\n",
    "\n",
    "    # validation\n",
    "    # metric_list = 0.0\n",
    "    folder_name = os.path.join(\"see_image\", \"val\")\n",
    "    model_s.eval()\n",
    "    for batch in tqdm(val_loader):\n",
    "        image_val, label, id_val = batch\n",
    "        image_val, label, id_val = image_val.to(device), label.to(device), id_val\n",
    "\n",
    "        H, W = label.size(1), label.size(2)\n",
    "        up_sizes = (ceil(H / 8) * 8, ceil(W / 8) * 8)\n",
    "\n",
    "        for i in range(0, int(label.size(0))):\n",
    "            folder_val = os.path.join(folder_name, \"val_original_target\")\n",
    "            os.makedirs(folder_val, exist_ok=True)\n",
    "            image = Image.fromarray(np.uint8(label[i].detach().cpu().numpy()))\n",
    "            image.save(os.path.join(folder_val, str(id_val[i]) + \".png\"))\n",
    "\n",
    "        data = torch.nn.functional.interpolate(image_val, size=(up_sizes[0], up_sizes[1]),\n",
    "                                               mode='bilinear', align_corners=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_t1, output = model_t1(input_ul=data,  target_ul=label)\n",
    "            \n",
    "            output = torch.nn.functional.interpolate(output, size=(H, W),\n",
    "                                                 mode='bilinear', align_corners=True)\n",
    "\n",
    "        for i in range(0, int(output.size(0))):\n",
    "            folder_val_prob = os.path.join(folder_name, \"val_original_prob\")\n",
    "            os.makedirs(folder_val_prob, exist_ok=True)\n",
    "            image_prob = output[i].squeeze().detach()\n",
    "            image_prob = torch.argmax(image_prob, dim=0).cpu().numpy()\n",
    "            image_prob = Image.fromarray((image_prob * 255).astype(np.uint8))\n",
    "            image_prob.save(os.path.join(folder_val_prob, str(id_val[i]) + \".png\"))\n",
    "        \n",
    "\n",
    "    \n",
    "    # show epoch mIoU, mDice\n",
    "    index_mIoU, index_mDice = count_index(folder_val_prob, folder_val)\n",
    "    # print(f'Epoch {epoch+1}' + \" val:\" + f'DSC: {index_mDice:.4f}, HD95 = {index_mean_hd95:.4f}')\n",
    "    print(f'Epoch {epoch+1}' + \" val:\" + f'mIoU: {index_mIoU:.4f}, DSC: {index_mDice:.4f}')\n",
    "\n",
    "    # find the best mIoU, mDice\n",
    "    # if index_mDice > do_best_Dice and (index_mDice > do_best_Dice or index_mean_hd95 < do_best_hd95):\n",
    "    if index_mDice > do_best_Dice:\n",
    "        early_stopper.best_score = index_mDice\n",
    "        early_stopper.counter = 0\n",
    "        do_best_Dice = index_mDice\n",
    "        do_best_mIoU = index_mIoU\n",
    "        # do_best_hd95 = index_mean_hd95\n",
    "        do_best_epoch = epoch+1\n",
    "        best_model_t1_params = copy.deepcopy(model_t1.state_dict())\n",
    "        best_model_s_params = copy.deepcopy(model_s.state_dict())\n",
    "        # print(\"Change Best: \" + f'epoch: {do_best_epoch} DSC: {do_best_Dice:.4f}, HD95 = {do_best_hd95:.4f}')\n",
    "        print(\"Change Best: \" + f'epoch : {do_best_epoch} mIoU: {do_best_mIoU:.4f}, DSC: {do_best_Dice:.4f}')\n",
    "\n",
    "        # save the best valiation prod\n",
    "        folder_val_best_prob = os.path.join(folder_name, \"val_original_best_prob\")\n",
    "        os.makedirs(folder_val_best_prob, exist_ok=True)\n",
    "        file_names = os.listdir(folder_val_prob)\n",
    "        for file_name in file_names:\n",
    "            source_path = os.path.join(folder_val_prob, file_name)\n",
    "            destination_path = os.path.join(folder_val_best_prob, file_name)\n",
    "            shutil.copyfile(source_path, destination_path)\n",
    "    else:\n",
    "        if epoch >= 5:\n",
    "            early_stopper(index_mDice)\n",
    "    \n",
    "    if early_stopper.early_stop: \n",
    "        print(\"Early stopping\")   \n",
    "        break\n",
    "\n",
    "    # show the best mIoU, mDice\n",
    "    # print(\"Best model : \" + f'epoch : {do_best_epoch}  DSC : {do_best_Dice:.4f}, HD95 = {do_best_hd95:.4f}')\n",
    "    print(\"Best model : \" + f'epoch : {do_best_epoch} mIoU: {do_best_mIoU:.4f}, DSC: {do_best_Dice:.4f}')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best teacher1&2 and student model\n",
    "model_path = None\n",
    "model_path = os.path.join(\"saved_models\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "model_params = os.path.join(model_path, \"original_models\")\n",
    "os.makedirs(model_params, exist_ok=True)\n",
    "\n",
    "torch.save(best_model_t1_params, os.path.join(model_params, f'original_epoch_{do_best_epoch}_dsc_{do_best_Dice:.4f}_best_t1.pth'))\n",
    "torch.save(best_model_s_params, os.path.join(model_params, f'original_epoch_{do_best_epoch}_dsc_{do_best_Dice:.4f}_best_s.pth'))\n",
    "print(\"Best model : \" + f'epoch : {do_best_epoch}  DSC : {do_best_Dice:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps-mt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
